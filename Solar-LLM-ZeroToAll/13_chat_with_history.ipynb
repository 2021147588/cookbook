{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/UpstageAI/cookbook/blob/main/Solar-LLM-ZeroToAll/01_hello_solar.ipynb\">\n",
    "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip3 install -qU langchain-upstage  python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "# set UPSTAGE_API_KEY in .env file\n",
    "# UPSTAGE_API_KEY=your_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_upstage import ChatUpstage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "\n",
    "llm = ChatUpstage()\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an assistant for question-answering tasks. \"),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = qa_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ah, Korea! A fascinating country with a rich history and culture. The Korean Peninsula is home to two distinct countries: North Korea and South Korea. Both countries have their own unique characteristics and attractions.\n",
      "\n",
      "South Korea, officially known as the Republic of Korea, is a vibrant and modern country with a strong economy. It's known for its bustling cities like Seoul, which offers a mix of traditional and modern architecture, cuisine, and technology. South Korea is also famous for its K-pop music, dramas, and movies, which have gained global popularity in recent years.\n",
      "\n",
      "North Korea, officially known as the Democratic People's Republic of Korea, is a more closed-off country with a different political system. It's less known for its tourist attractions but does have some unique offerings such as the capital city Pyongyang, the Demilitarized Zone (DMZ) between North and South Korea, and the ancient city of Kaesong.\n",
      "\n",
      "Both countries have beautiful landscapes, from mountains and beaches to temples and palaces. Korean cuisine is also quite diverse and delicious, featuring dishes like kimchi, bibimbap, and bulgogi.\n",
      "\n",
      "If you're planning a trip to Korea, it's essential to research and understand the differences between the two countries and their respective visa requirements and travel restrictions.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "question = \"How about Korea?\"\n",
    "ai_msg_1 = chain.invoke({\"input\": question, \"chat_history\": []})\n",
    "print(ai_msg_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n",
      "The capital of South Korea is Seoul.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "question = \"Where is the capital of France?\"\n",
    "ai_msg_1 = chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "print(ai_msg_1)\n",
    "chat_history += [HumanMessage(question), AIMessage(ai_msg_1)]\n",
    "\n",
    "\n",
    "second_question = \"How about Korea?\"\n",
    "ai_msg_2 = chain.invoke({\"input\": second_question, \"chat_history\": chat_history})\n",
    "chat_history += [HumanMessage(second_question), AIMessage(ai_msg_2)]\n",
    "\n",
    "print(ai_msg_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatUpstage()\n",
    "\n",
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "Use three sentences maximum and keep the answer concise.\\\n",
    "\n",
    "{context}\"\"\"\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = qa_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "We introduce SOLAR 10.7B, a large language model (LLM) with 10.7 billion parameters, \n",
    "    demonstrating superior performance in various natural language processing (NLP) tasks. \n",
    "    Inspired by recent efforts to efficiently up-scale LLMs, \n",
    "    we present a method for scaling LLMs called depth up-scaling (DUS), \n",
    "    which encompasses depthwise scaling and continued pretraining.\n",
    "    In contrast to other LLM up-scaling methods that use mixture-of-experts, \n",
    "    DUS does not require complex changes to train and inference efficiently. \n",
    "    We show experimentally that DUS is simple yet effective \n",
    "    in scaling up high-performance LLMs from small ones. \n",
    "    Building on the DUS model, we additionally present SOLAR 10.7B-Instruct, \n",
    "    a variant fine-tuned for instruction-following capabilities, \n",
    "    surpassing Mixtral-8x7B-Instruct. \n",
    "    SOLAR 10.7B is publicly available under the Apache 2.0 license, \n",
    "    promoting broad access and application in the LLM field.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A1 DUS stands for depth up-scaling, which is a method for scaling large language models (LLMs) introduced in the context. It encompasses depthwise scaling and continued pretraining, and is a simple yet effective way to scale up high-performance LLMs from small ones.\n",
      "A2 The benefit of DUS is that it allows for the efficient scaling of high-performance LLMs from smaller models, without requiring complex changes to the training or inference process. This method is simpler than other LLM up-scaling methods that use mixture-of-experts, and it demonstrates superior performance in various natural language processing tasks.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "question = \"Wat is DUS?\"\n",
    "ai_msg_1 = chain.invoke({\"input\": question, \"chat_history\": chat_history, \"context\": context})\n",
    "chat_history += [HumanMessage(question), AIMessage(ai_msg_1)]\n",
    "print(\"A1\", ai_msg_1)\n",
    "\n",
    "second_question = \"What's the benefit?\"\n",
    "ai_msg_2 = chain.invoke({\"input\": second_question, \"chat_history\": chat_history, \"context\": context})\n",
    "chat_history += [HumanMessage(second_question), AIMessage(ai_msg_2)]\n",
    "\n",
    "print(\"A2\", ai_msg_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Wat is DUS?'\n",
      "content='DUS stands for depth up-scaling, which is a method for scaling large language models (LLMs) introduced in the context. It encompasses depthwise scaling and continued pretraining, and is a simple yet effective way to scale up high-performance LLMs from small ones.'\n",
      "content=\"What's the benefit?\"\n",
      "content='The benefit of DUS is that it allows for the efficient scaling of high-performance LLMs from smaller models, without requiring complex changes to the training or inference process. This method is simpler than other LLM up-scaling methods that use mixture-of-experts, and it demonstrates superior performance in various natural language processing tasks.'\n"
     ]
    }
   ],
   "source": [
    "for chat in chat_history:\n",
    "    print(chat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
