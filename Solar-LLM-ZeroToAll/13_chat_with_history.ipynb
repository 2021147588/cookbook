{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/UpstageAI/cookbook/blob/main/Solar-LLM-ZeroToAll/01_hello_solar.ipynb\">\n",
    "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip3 install -qU langchain-upstage  python-dotenv getpass4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "UPSTAGE_API_KEY = getpass.getpass('Enter your API Key')\n",
    "_ = os.environ.setdefault(\"UPSTAGE_API_KEY\", UPSTAGE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_upstage import ChatUpstage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "\n",
    "llm = ChatUpstage()\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an assistant for question-answering tasks. \"),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = qa_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Korea, divided into North and South, is a region with a rich history and diverse culture. The Korean Peninsula is located in East Asia, bordered by China to the northwest, Russia to the northeast, and Japan to the east.\n",
      "\n",
      "South Korea, officially the Republic of Korea, is a constitutional democracy with a strong economy and a highly educated population. It is known for its vibrant capital city, Seoul, which is a hub for technology, fashion, and pop culture. South Korea has a strong industrial base and is home to several global companies like Samsung and Hyundai. The country also has a rich cultural heritage, with traditions such as K-pop, K-dramas, and traditional Korean cuisine gaining worldwide popularity.\n",
      "\n",
      "North Korea, officially the Democratic People's Republic of Korea, is a single-party state under the control of the Kim family since its founding in 1948. It is one of the most isolated and secretive countries in the world, with a centralized economy and a strict control over information and movement. The country has been involved in several international conflicts and is known for its nuclear weapons program.\n",
      "\n",
      "Both North and South Korea have a unique blend of cultural influences, with traditions rooted in Confucianism, Buddhism, and shamanism. Korean cuisine, architecture, literature, and arts have evolved over centuries and continue to be an important part of the country's identity.\n",
      "\n",
      "In summary, Korea is a region with a complex history and diverse cultural heritage, with South Korea being a modern, industrialized democracy and North Korea a highly isolated, authoritarian state.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "question = \"How about Korea?\"\n",
    "ai_msg_1 = chain.invoke({\"input\": question, \"chat_history\": []})\n",
    "print(ai_msg_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list.extend() takes exactly one argument (2 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m second_question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow about Korea?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m ai_msg_2 \u001b[38;5;241m=\u001b[39m chain\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: second_question, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m\"\u001b[39m: chat_history})\n\u001b[0;32m---> 13\u001b[0m \u001b[43mchat_history\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43msecond_question\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mAIMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mai_msg_2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(ai_msg_2)\n",
      "\u001b[0;31mTypeError\u001b[0m: list.extend() takes exactly one argument (2 given)"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "question = \"Where is the capital of France?\"\n",
    "ai_msg_1 = chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "print(ai_msg_1)\n",
    "chat_history.extend([HumanMessage(question), AIMessage(ai_msg_1)])\n",
    "\n",
    "\n",
    "second_question = \"How about Korea?\"\n",
    "ai_msg_2 = chain.invoke({\"input\": second_question, \"chat_history\": chat_history})\n",
    "chat_history.extend(HumanMessage(second_question), AIMessage(ai_msg_2))\n",
    "\n",
    "print(ai_msg_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatUpstage()\n",
    "\n",
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "Use three sentences maximum and keep the answer concise.\\\n",
    "\n",
    "{context}\"\"\"\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = qa_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "We introduce SOLAR 10.7B, a large language model (LLM) with 10.7 billion parameters, \n",
    "    demonstrating superior performance in various natural language processing (NLP) tasks. \n",
    "    Inspired by recent efforts to efficiently up-scale LLMs, \n",
    "    we present a method for scaling LLMs called depth up-scaling (DUS), \n",
    "    which encompasses depthwise scaling and continued pretraining.\n",
    "    In contrast to other LLM up-scaling methods that use mixture-of-experts, \n",
    "    DUS does not require complex changes to train and inference efficiently. \n",
    "    We show experimentally that DUS is simple yet effective \n",
    "    in scaling up high-performance LLMs from small ones. \n",
    "    Building on the DUS model, we additionally present SOLAR 10.7B-Instruct, \n",
    "    a variant fine-tuned for instruction-following capabilities, \n",
    "    surpassing Mixtral-8x7B-Instruct. \n",
    "    SOLAR 10.7B is publicly available under the Apache 2.0 license, \n",
    "    promoting broad access and application in the LLM field.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A1 DUS stands for depth up-scaling, which is a method for scaling large language models (LLMs) by encompassing depthwise scaling and continued pretraining. It is a simple yet effective technique for scaling up high-performance LLMs from small ones, and it does not require complex changes to train and inference efficiently.\n",
      "A2 The benefit of DUS is that it allows for the scaling of large language models (LLMs) without requiring complex changes to the training and inference process. This scaling can lead to improved performance in various natural language processing (NLP) tasks. Additionally, DUS enables the creation of high-performance LLMs from smaller ones, which can be particularly useful in the development of advanced AI systems.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "question = \"Wat is DUS?\"\n",
    "ai_msg_1 = chain.invoke({\"input\": question, \"chat_history\": chat_history, \"context\": context})\n",
    "chat_history += [HumanMessage(question), AIMessage(ai_msg_1)]\n",
    "print(\"A1\", ai_msg_1)\n",
    "\n",
    "second_question = \"What's the benefit?\"\n",
    "ai_msg_2 = chain.invoke({\"input\": second_question, \"chat_history\": chat_history, \"context\": context})\n",
    "chat_history += [HumanMessage(second_question), AIMessage(ai_msg_2)]\n",
    "\n",
    "print(\"A2\", ai_msg_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Wat is DUS?'\n",
      "content='DUS stands for depth up-scaling, which is a method for scaling large language models (LLMs) by encompassing depthwise scaling and continued pretraining. It is a simple yet effective technique for scaling up high-performance LLMs from small ones, and it does not require complex changes to train and inference efficiently.'\n",
      "content=\"What's the benefit?\"\n",
      "content='The benefit of DUS is that it allows for the scaling of large language models (LLMs) without requiring complex changes to the training and inference process. This scaling can lead to improved performance in various natural language processing (NLP) tasks. Additionally, DUS enables the creation of high-performance LLMs from smaller ones, which can be particularly useful in the development of advanced AI systems.'\n"
     ]
    }
   ],
   "source": [
    "for chat in chat_history:\n",
    "    print(chat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
