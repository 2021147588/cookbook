{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upstage Full Stack LLM with Langchain\n",
    "## Code to Understand!\n",
    "![Overview](./figures/overview.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "! pip3 install -qU guardrails-ai openai langchain_community langchain_experimental langchain-upstage sentence-transformers langchainhub langchain-chroma langchain matplotlib python-dotenv tavily-python ragas faiss-cpu tokenizers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UPSTAGE_API_KEY\n",
    "To obtain your Upstage API key, follow these steps:\n",
    "\n",
    "1. Visit the Upstage AI console at <https://console.upstage.ai>.\n",
    "2. Sign up for an account if you don't already have one.\n",
    "3. Log in to your account.\n",
    "4. Navigate to the API key section.\n",
    "5. Generate your API key.\n",
    "6. Copy the key and save it securely.\n",
    "\n",
    "![Console](./figures/console.upstage.ai.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "source": [
    "# Set .env and define these:\n",
    "# UPSTAGE_API_KEY from https://console.upstage.ai/\n",
    "# TAVILY_API_KEY https://app.tavily.com\n",
    "# NEWS_API_KEY from https://newsapi.org/\n",
    "\n",
    "%load_ext dotenv\n",
    "%dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting with the Solar-1-mini-chat Model\n",
    "\n",
    "This Python code demonstrates how to use the OpenAI API to interact with the Solar-1-mini-chat model provided by Upstage AI.\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. Import necessary libraries: `os`, `openai`, and `pprint`.\n",
    "2. Set up the OpenAI client with the API key and base URL.\n",
    "3. Create a chat completion request using `client.chat.completions.create()`.\n",
    "   - Specify the model: \"solar-1-mini-chat\".\n",
    "   - Provide a list of messages, including the system message and user message.\n",
    "4. Handle the model's response:\n",
    "   - Print the entire response using `pprint()`.\n",
    "   - Print the content of the assistant's message using `response.choices[0].message.content`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='757e2cf2-7654-4916-9db3-591899e42dbb', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"I'm sorry, but I'm not sure what you're asking about Korea. Could you please provide more context or specify what you would like to know about Korea?\", role='assistant', function_call=None, tool_calls=None))], created=1718391224, model='solar-1-mini-chat-240612', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=37, prompt_tokens=26, total_tokens=63))\n",
      "Message only:\n",
      "(\"I'm sorry, but I'm not sure what you're asking about Korea. Could you please \"\n",
      " 'provide more context or specify what you would like to know about Korea?')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from pprint import pprint\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.environ[\"UPSTAGE_API_KEY\"], base_url=\"https://api.upstage.ai/v1/solar\"\n",
    ")\n",
    "chat_result = client.chat.completions.create(\n",
    "    model=\"solar-1-mini-chat\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What about Korea?\"},\n",
    "    ],\n",
    ")\n",
    "pprint(chat_result)\n",
    "print(\"Message only:\")\n",
    "pprint(chat_result.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Few-Shot Examples in Chat Completions\n",
    "\n",
    "This Python code demonstrates how to use few-shot examples in the OpenAI Chat Completions API to provide context and guide the model's responses.\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. Set up the OpenAI client with the API key and base URL.\n",
    "2. Create a chat completion request using `client.chat.completions.create()`.\n",
    "   - Specify the model: \"solar-1-mini-chat\".\n",
    "   - Provide a list of messages, including:\n",
    "     - System message: Defines the assistant's role.\n",
    "     - Few-shot examples: Provide context and desired behavior.\n",
    "     - User input: The actual user query.\n",
    "3. Handle the model's response:\n",
    "   - Print the entire response using `pprint()`.\n",
    "   - Print the content of the assistant's message using `response.choices[0].message.content`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='47d4be1f-28f4-4468-b6cb-5567404c58ad', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The capital of South Korea is Seoul.', role='assistant', function_call=None, tool_calls=None))], created=1718391225, model='solar-1-mini-chat-240612', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=10, prompt_tokens=55, total_tokens=65))\n",
      "Message only:\n",
      "'The capital of South Korea is Seoul.'\n"
     ]
    }
   ],
   "source": [
    "# few shots: examples or history\n",
    "chat_result = client.chat.completions.create(\n",
    "    model=\"solar-1-mini-chat\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        # examples\n",
    "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"I know of it. It's Paris!!\",\n",
    "        },\n",
    "        # user input\n",
    "        {\"role\": \"user\", \"content\": \"What about Korea?\"},\n",
    "    ],\n",
    ")\n",
    "pprint(chat_result)\n",
    "print(\"Message only:\")\n",
    "pprint(chat_result.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building LLM Applications with LangChain\n",
    "\n",
    "This Python code demonstrates how to use the LangChain library to build applications with Large Language Models (LLMs). It covers the basic steps of defining an LLM, creating a chat prompt, defining a chain, and invoking the chain.\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. Define your favorite LLM:\n",
    "   - Import the `ChatUpstage` class from `langchain_upstage`.\n",
    "   - Create an instance of `ChatUpstage` and assign it to the variable `llm`.\n",
    "\n",
    "2. Define a chat prompt:\n",
    "   - Import the `ChatPromptTemplate` class from `langchain_core.prompts`.\n",
    "   - Create a `ChatPromptTemplate` instance using the `from_messages()` method.\n",
    "   - Provide a list of messages, including system messages, example conversations, and user input.\n",
    "\n",
    "3. Define a chain:\n",
    "   - Import the `StrOutputParser` class from `langchain_core.output_parsers`.\n",
    "   - Create a chain by combining the `rag_with_history_prompt`, `llm`, and `StrOutputParser()` using the pipe (`|`) operator.\n",
    "\n",
    "4. Invoke the chain:\n",
    "   - Call the `invoke()` method on the `chain` object, passing an empty dictionary (`{}`) as the input.\n",
    "   - Print the response obtained from the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello! I'm just a computer program, so I don't have feelings or emotions. But I'm here to help you with any questions or problems you might have. Is there anything specific you would like to talk about or ask about?\", response_metadata={'token_usage': {'completion_tokens': 52, 'prompt_tokens': 16, 'total_tokens': 68}, 'model_name': 'solar-1-mini-chat', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-abd57b26-012f-4532-a0f1-c00d0f0cdce1-0')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quick hello world\n",
    "from langchain_upstage import ChatUpstage\n",
    "\n",
    "llm = ChatUpstage(base_url=\"https://40ff-211-51-79-107.ngrok-free.app/v1\")\n",
    "llm.invoke(\"Hello, how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oui, oui! Korea, Korea... I know this one too! The capital of South Korea is Seoul, and the capital of North Korea is Pyongyang. But I'm more interested in their cuisine, like Kimchi and BBQ. Mmm, delicious!\n"
     ]
    }
   ],
   "source": [
    "# langchain, 1. llm define, 2. prompt define, 3. chain, 4. chain.invoke\n",
    "\n",
    "# 1. define your favorate llm, solar\n",
    "from langchain_upstage import ChatUpstage\n",
    "\n",
    "llm = ChatUpstage()\n",
    "\n",
    "# 2. define chat prompt\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "rag_with_history_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"human\", \"What is the capital of France?\"),\n",
    "        (\"ai\", \"I know of it. It's Paris!!\"),\n",
    "        (\"human\", \"What about Korea?\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 3. define chain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chain = rag_with_history_prompt | llm | StrOutputParser()\n",
    "\n",
    "# 4. invoke the chain\n",
    "c_result = chain.invoke({})\n",
    "print(c_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameterized Prompt Templates in LangChain\n",
    "\n",
    "### Overview\n",
    "\n",
    "- Prompt templates allow for reusable and modular prompts\n",
    "- They improve maintainability compared to using raw prompt strings\n",
    "- {country} value can be set from outside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Another one I know! Seoul, the capital of South Korea.\n",
      "---\n",
      "I'm sorry, but I don't know the capital of Japan. I can help you with many things, but I don't have all the answers to everything.\n"
     ]
    }
   ],
   "source": [
    "# parameterized prompt template\n",
    "rag_with_history_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"human\", \"What is the capital of France?\"),\n",
    "        (\"ai\", \"I know of it. It's Paris!!\"),\n",
    "        (\"human\", \"What about {country}?\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = rag_with_history_prompt | llm | StrOutputParser()\n",
    "\n",
    "# 4. invoke chain with param\n",
    "print(chain.invoke({\"country\": \"Korea\"}))\n",
    "print(\"---\")\n",
    "print(chain.invoke({\"country\": \"Japan\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leveraging Message History in LangChain Prompts\n",
    "\n",
    "- LangChain provides powerful tools for managing conversation history\n",
    "- `MessagesPlaceholder` allows for dynamic inclusion of message history\n",
    "- `HumanMessage` and `AIMessage` classes represent individual messages\n",
    "- Combining message history with user input enables context-aware responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of South Korea is Seoul, while the capital of North Korea is Pyongyang.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# More general chat\n",
    "rag_with_history_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "history = [\n",
    "    HumanMessage(\"What is the capital of France?\"),\n",
    "    AIMessage(\"It's Paris!!\"),\n",
    "]\n",
    "\n",
    "chain = rag_with_history_prompt | llm | StrOutputParser()\n",
    "chain_result = chain.invoke({\"history\": history, \"input\": \"What about Korea?\"})\n",
    "print(chain_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Hallucination](./figures/hallucination.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Upstage DUS is a technique used in the field of computer vision and image processing. It stands for \"Dense Unsupervised Stereo,\" and it is a method for estimating the depth of a scene from a single image.\\n\\nThe Upstage DUS technique is based on the idea of using a convolutional neural network (CNN) to predict the depth of each pixel in an image. The network is trained on a large dataset of stereo pairs, which are pairs of images taken from slightly different viewpoints. By comparing the two images in each pair, the network can learn to estimate the depth of the scene.\\n\\nOnce the network has been trained, it can be used to estimate the depth of a new image by processing it through the network and producing a depth map. This depth map can then be used for a variety of applications, such as 3D reconstruction, augmented reality, and robotics.\\n\\nUpstage DUS is a dense technique, which means that it can estimate the depth of every pixel in an image, rather than just a few select points. This makes it useful for applications that require high-resolution depth information.', response_metadata={'token_usage': {'completion_tokens': 242, 'prompt_tokens': 18, 'total_tokens': 260}, 'model_name': 'solar-1-mini-chat', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-a40c5655-b584-448a-a1eb-ca7205c73a34-0')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cannot say \"I don't know\" :-)\n",
    "# Because it is trained to complete the sentence and try to answer the question\n",
    "llm.invoke(\"What is Upstage DUS technique?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG\n",
    "Provide context and allow the language model to respond within that context only.\n",
    "![Overview](./figures/rag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leveraging Layout Analyzer and LangChain for Efficient Text Splitting and Vectorization\n",
    "\n",
    "- Upstage Layout Analyzer extracts layouts, tables, and figures from any document\n",
    "- LangChain provides powerful tools for text splitting and vectorization\n",
    "![Layout Analyzer](./figures/la.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_upstage import (\n",
    "    UpstageLayoutAnalysisLoader,\n",
    "    UpstageGroundednessCheck,\n",
    "    ChatUpstage,\n",
    "    UpstageEmbeddings,\n",
    ")\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "layzer = UpstageLayoutAnalysisLoader(\"pdfs/solar_paper.pdf\", output_type=\"html\")\n",
    "# For improved memory efficiency, consider using the lazy_load method to load documents page by page.\n",
    "docs = layzer.load()  # or layzer.lazy_load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"<p id='0' style='font-size:22px'>SOLAR 10.7B: Scaling Large Language Models \"\n",
      " 'with Simple yet Effectiv')\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    pprint(doc.page_content[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p id='0' style='font-size:22px'>SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective<br>Depth Up-Scaling</p><p id='1' style='font-size:20px'>Dahyun Kim* , Chanjun Park*†, Sanghoon Kim*†, Wonsung Lee*†, Wonho Song*<br>Yunsu Kim* Hyeonwoo Kim* , Yungi Kim, Hyeonju Lee, Jihoo Kim<br>,<br>Changbae Ahn, Seonghoon Yang, Sukyung Lee, Hyunbyung Park, Gyoungjin Gim<br>Mikyoung Cha, Hwalsuk Leet , Sunghun Kim t</p><p id='2' style='font-size:20px'>Upstage AI, South Korea</p><p id='3' style='font-size:14px'>{ kdahyun, chan jun . park, limerobot, wonsung · lee, hwalsuk lee, hunkim} @upstage . ai</p><br><p id='4' style='font-size:18px'>Abstract</p><p id='5' style='font-size:16px'>We introduce SOLAR 10.7B, a large language<br>model (LLM) with 10.7 billion parameters,<br>demonstrating superior performance in various<br>natural language processing (NLP) tasks. In-<br>spired by recent efforts to efficiently up-scale<br>LLMs, we present a method for scaling LLMs<br>called depth up-scalin"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(docs[0].page_content[:1000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Generation (RAG) for Question Answering\n",
    "\n",
    "- RAG combines retrieval and generation to enhance LLM performance on specific tasks\n",
    "- Relevant context is retrieved from external data sources and added to the prompt\n",
    "- The augmented prompt is then passed to the LLM for generating a response\n",
    "- RAG is particularly useful for question answering on custom datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESPONSE1\n",
      " The table presents the performance comparison of two merge candidates, named 'Cand. 1' and 'Cand. 2'. 'Cand. 1' has high GSM8K scores but relatively low scores for the other tasks, while 'Cand. 2' has low scores for GSM8K but high scores for the other tasks. This suggests that 'Cand. 1' is more specialized in GSM8K tasks, while 'Cand. 2' is more well-rounded in performance across different tasks.\n"
     ]
    }
   ],
   "source": [
    "# More general chat\n",
    "rag_with_history_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question considering the history of the conversation. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "---\n",
    "CONTEXT:\n",
    "{context}\n",
    "         \"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "history = []\n",
    "\n",
    "chain = rag_with_history_prompt | llm | StrOutputParser()\n",
    "query1 = \"Performance comparison amongst the merge candidate\"\n",
    "response1 = chain.invoke({\"history\": history, \"context\": docs, \"input\": query1})\n",
    "print(\"RESPONSE1\\n\", response1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESPONSE2\n",
      " Ablation studies are a type of experimental design used in scientific research to understand the effect of a particular variable or component on the overall system or model. In the context of machine learning and artificial intelligence, ablation studies are often used to test the importance of specific features, layers, or components in a model.\n",
      "\n",
      "The idea behind ablation studies is to remove or \"ablate\" a specific part of the model and observe the change in performance. If the performance of the model decreases significantly after ablating a certain component, it suggests that the component is important for the model's performance. Conversely, if the performance remains relatively unchanged, it suggests that the component may not be critical.\n",
      "\n",
      "Ablation studies can be used to optimize models by identifying which components or features are most important and should be prioritized. They can also help researchers understand the inner workings of complex models by revealing which components or features contribute most to the model's performance.\n",
      "\n",
      "In the context of large language models like the one described in the paper, ablation studies could be used to test the effectiveness of different training datasets, model architectures, or fine-tuning strategies. For example, an ablation study might involve training a model with a subset of the training data and then comparing its performance to a model trained with the full dataset.\n",
      "\n",
      "Overall, ablation studies are a valuable tool for understanding the components and features that contribute to the performance of complex models, and can help researchers optimize and improve these models.\n"
     ]
    }
   ],
   "source": [
    "history = [HumanMessage(query1), AIMessage(response1)]\n",
    "query2 = \"How about Ablation studies?\"\n",
    "response2 = chain.invoke({\"history\": history, \"context\": docs, \"input\": query2})\n",
    "print(\"RESPONSE2\\n\", response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load something big\n",
    "layzer = UpstageLayoutAnalysisLoader(\n",
    "    \"pdfs/kim-tse-2008.pdf\", output_type=\"html\", use_ocr=True\n",
    ")\n",
    "# For improved memory efficiency, consider using the lazy_load method to load documents page by page.\n",
    "docs = layzer.load()  # or layzer.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  RAG Limitations\n",
    "- LLM does not have long enough context length\n",
    "- Sending long, irrelevant info is inefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 32768 tokens. However, your messages resulted in 34511 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n"
     ]
    }
   ],
   "source": [
    "chain = rag_with_history_prompt | llm | StrOutputParser()\n",
    "query1 = \"What is bug classification?\"\n",
    "\n",
    "try:\n",
    "    response1 = chain.invoke({\"history\": history, \"context\": docs, \"input\": query1})\n",
    "    print(response1)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101878\n"
     ]
    }
   ],
   "source": [
    "print(len(docs[0].page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer.from_pretrained(\"upstage/solar-1-mini-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded input: ['<|startoftext|>', '▁Nice', '▁to', '▁meet', '▁you', '.', '▁I', '▁am', '▁Solar', '▁LL', 'M', ',', '▁a', '▁large', '▁language', '▁model', '▁developed', '▁by', '▁Up', 'stage', '.', '▁If', '▁you', '▁have', '▁any', '▁questions', ',', '▁please', '▁feel', '▁free', '▁to', '▁ask', '.']\n",
      "Number of tokens: 33\n"
     ]
    }
   ],
   "source": [
    "text = \"Nice to meet you. I am Solar LLM, a large language model developed by Upstage. If you have any questions, please feel free to ask.\"\n",
    "\n",
    "enc = tokenizer.encode(text)\n",
    "print(\"Encoded input:\", enc.tokens)\n",
    "\n",
    "number_of_tokens = len(enc.tokens)\n",
    "print(\"Number of tokens:\", number_of_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded input: ['<|startoftext|>', '▁만나', '서', '▁반가', '워', '요', '.', '▁저는', '▁Up', 'stage', '에서', '▁개발한', '▁대규모', '▁언어', '▁모델', '인', '▁Solar', '▁LL', 'M', '▁입니다', '.', '▁궁금한', '▁것이', '▁있으', '시면', '▁무엇이', '든', '▁물어', '보세요', '.']\n",
      "Number of tokens: 30\n"
     ]
    }
   ],
   "source": [
    "text = \"만나서 반가워요. 저는 Upstage에서 개발한 대규모 언어 모델인 Solar LLM 입니다. 궁금한 것이 있으시면 무엇이든 물어보세요.\"\n",
    "enc = tokenizer.encode(text)\n",
    "print(\"Encoded input:\", enc.tokens)\n",
    "\n",
    "number_of_tokens = len(enc.tokens)\n",
    "print(\"Number of tokens:\", number_of_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded input: ['<|startoftext|>', '▁만나', '서', '▁반가', '워', '요', '.', '▁저는', '▁Up', 'stage', '에서', '▁개발한', '▁대규모', '▁언어', '▁모델', '인', '▁Solar', '▁LL', 'M', '▁입니다', '.', '▁궁금한', '▁것이', '▁있으', '시면', '▁무엇이', '든', '▁물어', '보세요', '.']\n",
      "Number of tokens: 30\n"
     ]
    }
   ],
   "source": [
    "enc = tokenizer.encode(text)\n",
    "print(\"Encoded input:\", enc.tokens)\n",
    "\n",
    "number_of_tokens = len(enc.tokens)\n",
    "print(\"Number of tokens:\", number_of_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_of_tokens(text):\n",
    "    return len(tokenizer.encode(text).tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENG 33\n",
      "KOR 30\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"ENG\",\n",
    "    num_of_tokens(\n",
    "        \"Nice to meet you. I am Solar LLM, a large language model developed by Upstage. If you have any questions, please feel free to ask.\"\n",
    "    ),\n",
    ")\n",
    "print(\n",
    "    \"KOR\",\n",
    "    num_of_tokens(\n",
    "        \"만나서 반가워요. 저는 Upstage에서 개발한 대규모 언어 모델인 Solar LLM 입니다. 궁금한 것이 있으시면 무엇이든 물어보세요.\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String length 101878\n",
      "Number of tokens 33529\n"
     ]
    }
   ],
   "source": [
    "# Recall\n",
    "# Let's load something big\n",
    "# layzer = UpstageLayoutAnalysisLoader(\"pdfs/kim-tse-2008.pdf\", output_type=\"html\")\n",
    "# For improved memory efficiency, consider using the lazy_load method to load documents page by page.\n",
    "# docs = layzer.load()  # or layzer.lazy_load()\n",
    "print(\"String length\", len(docs[0].page_content))\n",
    "print(\"Number of tokens\", num_of_tokens(docs[0].page_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficient Text Splitting and Indexing with LangChain\n",
    "\n",
    "### 1. Load Documents\n",
    "\n",
    "The first step is to load the source documents that will be used to augment the language model's knowledge\n",
    "This could be done by reading files from disk, pulling from a database, scraping web pages, etc.\n",
    "The goal is to get the raw text content into a format that can be further processed\n",
    "\n",
    "### 2. Chunking/Splitting\n",
    "\n",
    "* Long documents need to be broken down into smaller chunks that are a manageable size for embedding and retrieval\n",
    "Common approaches include:\n",
    "  * Fixed-size chunking - split text into equal sized chunks based on character or token count \n",
    "  * Semantic chunking - split based on semantic boundaries like sentences, paragraphs, or sections\n",
    "  * Hierarchical chunking - create chunks at multiple levels of granularity\n",
    "The ideal chunk size depends on the embedding model, retrieval use case, and downstream task\n",
    "\n",
    "### 3. Embedding & Indexing\n",
    "\n",
    "* The text chunks are converted to vector embeddings using a model like Upstage embeddings\n",
    "* The embeddings are indexed and stored in a vector database to enable efficient similarity search \n",
    "* Metadata about the source chunks can also be stored alongside the embeddings\n",
    "\n",
    "### 4. Retrieval\n",
    "\n",
    "* At query time, the user's question is itself embedded as a query vector\n",
    "* The query embedding is used to find the most similar document chunks in the vector index \n",
    "* Top-k most relevant chunks are retrieved and can be used to augment the prompt sent to the language model to generate an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits: 122\n"
     ]
    }
   ],
   "source": [
    "# RAG 1. load doc (done), 2. chunking, splits, 3. embeding - indexing, 4. retrieve\n",
    "\n",
    "# 2. Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "print(\"Splits:\", len(splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "split_lengths = [len(split.page_content) for split in splits]\n",
    "\n",
    "# Create a bar graph\n",
    "plt.bar(range(len(split_lengths)), split_lengths)\n",
    "plt.title(\"RecursiveCharacterTextSplitter\")\n",
    "plt.xlabel(\"Split Index\")\n",
    "plt.ylabel(\"Split Content Length\")\n",
    "plt.xticks(range(len(split_lengths)), [])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.2 s, sys: 1.57 s, total: 3.77 s\n",
      "Wall time: 32.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 3. Embed & indexing\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=UpstageEmbeddings(model=\"solar-embedding-1-large\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "<p id='2' style='font-size:22px'>Classifying Software Changes:<br>Clean or Buggy?</p><p id='3' style\n"
     ]
    }
   ],
   "source": [
    "# 4. retrive\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "result_docs = retriever.invoke(\"What is Bug Classification?\")\n",
    "print(len(result_docs))\n",
    "print(result_docs[0].page_content[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Overview](./figures/semantic_chunker.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-2. SemanticChunker Split\n",
    "from langchain_community.utils.math import cosine_similarity\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "\n",
    "def semantic_chunker(\n",
    "    docs,\n",
    "    min_chunk_size=100,\n",
    "    chunk_overlap=10,\n",
    "    max_chunk_size=1000,\n",
    "    merge_threshold=0.7,\n",
    "    embeddings=UpstageEmbeddings(model=\"solar-embedding-1-large\"),\n",
    "):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=min_chunk_size, chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    init_splits = text_splitter.split_documents(docs)\n",
    "    splits = []\n",
    "\n",
    "    base_split_text = None\n",
    "    base_split_emb = None\n",
    "    for split in init_splits:\n",
    "        if base_split_text is None:\n",
    "            base_split_text = split.page_content\n",
    "            base_split_emb = embeddings.embed_documents([base_split_text])[0]\n",
    "            continue\n",
    "\n",
    "        split_emb = embeddings.embed_documents([split.page_content])[0]\n",
    "        distance = cosine_similarity(X=[base_split_emb], Y=[split_emb])\n",
    "        if (\n",
    "            distance[0][0] < merge_threshold\n",
    "            or len(base_split_text) + len(split.page_content) > max_chunk_size\n",
    "        ):\n",
    "            splits.append(Document(page_content=base_split_text))\n",
    "            base_split_text = split.page_content\n",
    "            base_split_emb = split_emb\n",
    "        else:\n",
    "            base_split_text += split.page_content\n",
    "\n",
    "    if base_split_text:\n",
    "        splits.append(Document(page_content=base_split_text))\n",
    "\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFaceEmbeddings\n",
    "Since it's just an approximation, it's acceptable to use very light embedding models like KLUE, https://huggingface.co/klue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name klue/roberta-small. Creating a new one with mean pooling.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-small and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 900 ms, sys: 2.44 s, total: 3.34 s\n",
      "Wall time: 992 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "hfembeddings = HuggingFaceEmbeddings(model_name=\"klue/roberta-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SemanticChunker Splits: 232\n",
      "CPU times: user 7.08 s, sys: 1.05 s, total: 8.13 s\n",
      "Wall time: 9.96 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "semantic_splits = semantic_chunker(docs,  merge_threshold=0.8, embeddings=hfembeddings)\n",
    "print(\"SemanticChunker Splits:\", len(semantic_splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "split_lengths = [num_of_tokens(split.page_content) for split in semantic_splits]\n",
    "\n",
    "# Create a bar graph\n",
    "plt.bar(range(len(split_lengths)), split_lengths)\n",
    "plt.xlabel(\"Split Index\")\n",
    "plt.ylabel(\"Split Content Length\")\n",
    "plt.title(\"Semantic Chunker Split Page Content Lengths\")\n",
    "plt.xticks(range(len(split_lengths)), [])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChromaParallel Class: Parallel Document Embedding\n",
    "The ChromaParallel class is an extension of the Chroma class to enable parallel processing of document embedding and storage using multiple worker processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "\n",
    "class ChromaParallel(Chroma):\n",
    "\n",
    "    async def afrom_documents(documents, embedding, num_workers=2):\n",
    "        db = Chroma(embedding_function=embedding)\n",
    "        # create list of num_workers empty lists\n",
    "        doc_groups = [[] for _ in range(num_workers)]\n",
    "\n",
    "        for i in range(len(documents)):\n",
    "            doc_groups[i % num_workers].append(documents[i])\n",
    "\n",
    "        tasks = [db.aadd_documents(group) for group in doc_groups]\n",
    "        await asyncio.gather(*tasks)\n",
    "        return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content=\"<p id='2' style='font-size:22px'>Classifying Software Changes:<br>Clean or Buggy?</p><p id='3' style='font-size:22px'>Sunghun Kim, E. James Whitehead Jr., Member, IEEE, and Yi Zhang, Member, IEEE</p><p id='4' style='font-size:16px'>Abstract-This paper introduces a new technique for predicting latent software bugs, called change classification. Change<br>classification uses a machine learning classifier to determine whether a new software change is more similar to prior buggy changes or<br>clean changes. In this manner, change classification predicts the existence of bugs in software changes. The classifier is trained using<br>features (in the machine learning sense) extracted from the revision history of a software project stored in its software configuration<br>management repository. The trained classifier can classify changes as buggy or clean, with a 78 percent accuracy and a 60 percent<br>buggy change recall on average. Change classification has several desirable qualities: 1) The\" metadata={'total_pages': 16}\n",
      "Wall time: 20.24 sec\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import time\n",
    "\n",
    "now = time.time()\n",
    "# 3. Embed & indexing\n",
    "loop = asyncio.get_event_loop()\n",
    "semantic_vectorstore = await ChromaParallel.afrom_documents(\n",
    "    documents=semantic_splits,\n",
    "    embedding=UpstageEmbeddings(model=\"solar-embedding-1-large\"),\n",
    "    num_workers=3,\n",
    ")\n",
    "semantic_retriever = semantic_vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# 4. retrive\n",
    "result_docs = semantic_retriever.invoke(\"What is Bug Classification?\")\n",
    "print(result_docs[1])\n",
    "print(f\"Wall time: {time.time() - now:.2f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bug classification is a process in software development that involves categorizing software bugs based on their characteristics, severity, and impact on the software system. This process helps developers and quality assurance teams to prioritize and manage bugs more effectively.\n",
      "\n",
      "Bug classification typically works in the following steps:\n",
      "\n",
      "1. **Identification**: The first step is to identify the bugs in the software. This can be done through various methods such as testing, user feedback, or automated bug detection tools.\n",
      "\n",
      "2. **Triaging**: Once the bugs are identified, they are triaged to determine their severity and impact on the software. This involves assessing factors such as how frequently the bug occurs, how many users are affected, and the potential consequences of the bug.\n",
      "\n",
      "3. **Categorization**: Based on the triaging process, bugs are categorized into different classes or categories. These categories may include things like critical bugs, minor bugs, usability issues, performance issues, etc.\n",
      "\n",
      "4. **Prioritization**: After the bugs are categorized, they are prioritized based on their severity and impact. Critical bugs that severely affect the software's functionality or security are typically given higher priority for fixing.\n",
      "\n",
      "5. **Assignment and Resolution**: Once the bugs are prioritized, they are assigned to developers or engineers for resolution. The developers work on fixing the bugs, and once they are fixed, the bugs are retested to ensure they are resolved.\n",
      "\n",
      "6. **Closure and Reporting**: After the bugs are fixed and retested, they are marked as resolved. The resolution process is documented, and a report is generated to provide an overview of the bugs fixed, their impact, and the resolution process.\n",
      "\n",
      "By following a systematic bug classification process, software development teams can ensure that their resources are allocated efficiently, and critical issues are addressed promptly, leading to better software quality and user satisfaction.\n"
     ]
    }
   ],
   "source": [
    "# Finally query using RAG\n",
    "query = \"What is bug classification? How it works?\"\n",
    "result_docs = semantic_retriever.invoke(query)\n",
    "\n",
    "gc_result = chain.invoke({\"history\": history, \"context\": result_docs, \"input\": query})\n",
    "print(gc_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bug classification is beneficial for several reasons:\n",
      "\n",
      "1. **Prioritization**: By categorizing and prioritizing bugs based on their severity and impact, development teams can focus on fixing the most critical issues first, ensuring that the software remains stable and functional.\n",
      "\n",
      "2. **Efficiency**: A well-defined bug classification system helps developers and quality assurance teams to work more efficiently. It allows them to quickly identify and address the most pressing issues, reducing the time to market for software updates and improvements.\n",
      "\n",
      "3. **Transparency**: Bug classification provides a clear overview of the bugs in the software, making it easier for stakeholders to understand the status of the software and the progress made in resolving issues.\n",
      "\n",
      "4. **Quality Assurance**: By ensuring that bugs are categorized and prioritized correctly, quality assurance teams can better monitor the software's quality and ensure that it meets the required standards.\n",
      "\n",
      "5. **User Satisfaction**: By addressing critical bugs and issues, software developers can improve the user experience and satisfaction, which can lead to increased user adoption and loyalty.\n",
      "\n",
      "6. **Risk Management**: Bug classification helps in identifying and mitigating potential risks associated with software bugs. This can prevent more significant issues from arising, saving time and resources in the long run.\n",
      "\n",
      "In summary, bug classification is an essential process in software development that helps teams manage bugs more effectively, improve software quality, and ensure user satisfaction.\n"
     ]
    }
   ],
   "source": [
    "history = [HumanMessage(query), AIMessage(gc_result)]\n",
    "\n",
    "query = \"Why it is good?\"\n",
    "result_docs = semantic_retriever.invoke(query)\n",
    "\n",
    "gc_result = chain.invoke({\"history\": history, \"context\": result_docs, \"input\": query})\n",
    "print(gc_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Evaluation using Ragas \n",
    "![ragas](figures/ragas.png)\n",
    "RAGAS is a framework that helps evaluate Retrieval Augmented Generation (RAG) pipelines. We generate questions from context and retrieve from a vector database. Then, we measure retrieval precision and recall.\n",
    "\n",
    "We use solar llm to create questions and to judge them as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('{\"keyphrases\": [\"Software Changes\", \"Clean or Buggy\", \"Change '\n",
      " 'Classification\", \"Latent Software Bugs\", \"Machine Learning Classifier\", '\n",
      " '\"Software Configuration Management Repository\", \"Buggy Changes\", \"Clean '\n",
      " 'Changes\", \"Change Tracking System\", \"Code Inspections\", \"Unit Testing\", '\n",
      " '\"Static Analysis Tools\", \"File-Level Software Changes\", \"Support Vector '\n",
      " 'Machine (SVM)\", \"Text Classification\", \"Software Evolution History\", '\n",
      " '\"Feature Extraction\", \"Source Code\", \"Change Histories\", \"Classification '\n",
      " 'Experiments\", \"Corpus\", \"Individual Features\", \"Related Work\"]}')\n",
      "['What is the main focus of the paper \"Classifying Software Changes: Clean or '\n",
      " 'Buggy?\" and what is the new technique introduced in this paper?',\n",
      " 'How does change classification work in predicting latent software bugs in '\n",
      " 'file-level software changes?',\n",
      " 'What is the main purpose of change classification in software development, '\n",
      " 'and how does it differ from traditional bug prediction techniques?',\n",
      " 'What is the goal of change classification in the context of software '\n",
      " 'development, and how does it differ from traditional bug prediction '\n",
      " 'techniques?',\n",
      " 'What is the main goal of change classification, and how does it differ from '\n",
      " 'other bug-finding techniques?',\n",
      " 'What is the new technique for predicting latent software bugs introduced in '\n",
      " 'the paper, and how does it differ from traditional bug prediction '\n",
      " 'techniques?',\n",
      " 'What is the main contribution of the paper \"Classifying Software Changes: '\n",
      " 'Clean or Buggy?\" and what is the purpose of the change classification '\n",
      " 'technique presented in it?',\n",
      " 'What is the main insight behind the change classification technique for '\n",
      " 'predicting bugs in file-level software changes?',\n",
      " 'What is change classification and how does it relate to predicting latent '\n",
      " 'software bugs in file-level software changes?',\n",
      " 'What is the main insight behind the change classification technique for '\n",
      " 'predicting bugs in file-level software changes?']\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/explodinggradients/ragas/blob/main/src/ragas/testset/prompts.py\n",
    "from ragas.testset.prompts import keyphrase_extraction_prompt, seed_question_prompt\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "def generate_questions(docs, n=10):\n",
    "    questions = []\n",
    "    chain = llm | StrOutputParser()\n",
    "\n",
    "    # Extract keyphrases\n",
    "    context = docs[0].page_content[:10000]\n",
    "    ragas_prompt = keyphrase_extraction_prompt.format(text=context)\n",
    "    keyphrase_extraction_results = chain.invoke(ragas_prompt.prompt_str)\n",
    "    pprint(keyphrase_extraction_results)\n",
    "\n",
    "    for _ in range(n):\n",
    "        ragas_prompt = seed_question_prompt.format(\n",
    "            keyphrase=keyphrase_extraction_results, context=context\n",
    "        )\n",
    "        questions_results = chain.invoke(ragas_prompt.prompt_str)\n",
    "        questions.append(questions_results)\n",
    "    return questions\n",
    "\n",
    "\n",
    "questions = generate_questions(docs)\n",
    "pprint(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split_lengths 122\n",
      "semantic_split_lengths 232\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "print(\"split_lengths\", len(splits))\n",
    "print(\"semantic_split_lengths\", len(semantic_splits))\n",
    "\n",
    "# 124 VS 250\n",
    "\n",
    "size_vectorstore = FAISS.from_documents(\n",
    "    documents=splits, embedding=UpstageEmbeddings(model=\"solar-embedding-1-large\")\n",
    ")\n",
    "size_split_retriever = size_vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "semantic_vectorstore = await ChromaParallel.afrom_documents(\n",
    "    documents=semantic_splits,\n",
    "    embedding=UpstageEmbeddings(model=\"solar-embedding-1-large\"),\n",
    "    num_workers=3,\n",
    ")\n",
    "\n",
    "semantic_split_retriever = semantic_vectorstore.as_retriever(search_kwargs={\"k\": 6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.evolutions import DataRow\n",
    "from ragas.testset.generator import TestsetGenerator, TestDataset\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import Dataset\n",
    "\n",
    "size_split_data = {\n",
    "    \"question\": [],\n",
    "    \"answer\": [],\n",
    "    \"contexts\": [],\n",
    "    \"ground_truth\": [],\n",
    "}\n",
    "semantic_split_data = {\n",
    "    \"question\": [],\n",
    "    \"answer\": [],\n",
    "    \"contexts\": [],\n",
    "    \"ground_truth\": [],\n",
    "}\n",
    "\n",
    "\n",
    "def fill_data(data, question, retr):\n",
    "    results = retr.invoke(question)\n",
    "    context = [doc.page_content for doc in results]\n",
    "\n",
    "    # chain = rag_with_history_prompt | llm | StrOutputParser()\n",
    "    # answer = chain.invoke({\"history\": [], \"context\": context, \"input\": question})\n",
    "\n",
    "    data[\"question\"].append(question)\n",
    "    data[\"answer\"].append(\"\")\n",
    "    data[\"contexts\"].append(context)\n",
    "    data[\"ground_truth\"].append(\"\")\n",
    "\n",
    "\n",
    "for question in questions:\n",
    "    fill_data(size_split_data, question, size_split_retriever)\n",
    "    fill_data(semantic_split_data, question, semantic_split_retriever)\n",
    "\n",
    "\n",
    "size_split_dataset = Dataset.from_dict(size_split_data)\n",
    "semantic_split_dataset = Dataset.from_dict(semantic_split_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "from ragas import evaluate\n",
    "\n",
    "\n",
    "def ragas_evalate(dataset):\n",
    "    result = evaluate(\n",
    "        dataset,\n",
    "        metrics=[\n",
    "            context_precision,\n",
    "            context_recall,\n",
    "            # answer_relevancy,\n",
    "            # faithfulness,\n",
    "        ],\n",
    "        llm=llm,\n",
    "        embeddings=UpstageEmbeddings(model=\"solar-embedding-1-large\"),\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 20/20 [00:07<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size splits {'context_precision': 1.0000, 'context_recall': 1.0000}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 20/20 [00:12<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic splits {'context_precision': 0.8533, 'context_recall': 1.0000}\n"
     ]
    }
   ],
   "source": [
    "print(\"Size splits\", ragas_evalate(size_split_dataset))\n",
    "print(\"Semantic splits\", ragas_evalate(semantic_split_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of the Code: Query Expander\n",
    "\n",
    "The provided code demonstrates a query expansion technique used in Retrieval Augmented Generation (RAG) systems. The main goal is to generate multiple variations of a given user query to retrieve relevant documents from a vector database more effectively. By generating different perspectives on the user query, the system aims to overcome some limitations of distance-based similarity search.\n",
    "\n",
    "The code defines a function called `query_expander` that takes a user query as input and returns a list of expanded queries. It uses three different query expansion templates:\n",
    "\n",
    "1. Multi Query: Generates five different versions of the user query to retrieve relevant documents from different perspectives.\n",
    "2. RAG-Fusion: Generates four related search queries based on the input query.\n",
    "3. Decomposition: Breaks down the input query into three sub-questions that can be answered in isolation.\n",
    "\n",
    "The expanded queries are generated using the LangChain library, specifically the `ChatUpstage` model, and the results are parsed using the `StrOutputParser`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1. Can you explain the DUS methodology used by Upstage?',\n",
      " '2. What does the DUS framework developed by Upstage entail?',\n",
      " '3. How does the DUS approach by Upstage differ from other similar methods?',\n",
      " '4. Can you provide an overview of the DUS strategy employed by Upstage?',\n",
      " '5. What are the key components of the DUS system developed by Upstage?',\n",
      " '1. What is the DUS approach in Upstage?',\n",
      " '2. How does the DUS approach work in Upstage?',\n",
      " '3. What are the key components of the DUS approach in Upstage?',\n",
      " '4. How can the DUS approach be implemented in Upstage?',\n",
      " '1. What is the DUS approach developed by Upstage?',\n",
      " '2. How does the DUS approach developed by Upstage work?',\n",
      " '3. What are the key features of the DUS approach developed by Upstage?']\n"
     ]
    }
   ],
   "source": [
    "from langchain_upstage import ChatUpstage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "def query_expander(query):\n",
    "    # Multi Query: Different Perspectives\n",
    "    multi_query_template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "    different versions of the given user question to retrieve relevant documents from a vector \n",
    "    database. By generating multiple perspectives on the user question, your goal is to help\n",
    "    the user overcome some of the limitations of the distance-based similarity search. \n",
    "    Provide these alternative questions separated by newlines. Original question: {query}\"\"\"\n",
    "\n",
    "    # RAG-Fusion: Related\n",
    "    rag_fusion_template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "    Generate multiple search queries related to: {query} \\n\n",
    "    Output (4 queries):\"\"\"\n",
    "\n",
    "    # Decomposition\n",
    "    decomposition_template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "    The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
    "    Generate multiple search queries related to: {query} \\n\n",
    "    Output (3 queries):\"\"\"\n",
    "\n",
    "    query_expander_templates = [\n",
    "        multi_query_template,\n",
    "        rag_fusion_template,\n",
    "        decomposition_template,\n",
    "    ]\n",
    "\n",
    "    expanded_queries = []\n",
    "    for template in query_expander_templates:\n",
    "        prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "        generate_queries = (\n",
    "            prompt_perspectives\n",
    "            | ChatUpstage(temperature=0)\n",
    "            | StrOutputParser()\n",
    "            | (lambda x: x.split(\"\\n\"))\n",
    "        )\n",
    "        expanded_queries += generate_queries.invoke({\"query\": query})\n",
    "\n",
    "    return expanded_queries\n",
    "\n",
    "\n",
    "expanded_queries = query_expander(\"What is the DUS approach developed by Upstage?\")\n",
    "pprint(expanded_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search for:  1. How do we categorize software bugs? What are the benefits of bug classification?\n",
      "Search for:  2. What is the process of sorting software defects? What are the advantages of bug classification?\n",
      "Search for:  3. How do we group software errors? What are the positive aspects of bug classification?\n",
      "Search for:  4. What is the methodology behind organizing software glitches? What are the merits of bug classification?\n",
      "Search for:  5. How do we segregate software faults? What are the positive outcomes of bug classification?\n",
      "Search for:  1. What is bug classification in software development?\n",
      "Search for:  2. How does bug classification help in software development?\n",
      "Search for:  3. What are the benefits of bug classification for software developers?\n",
      "Search for:  4. How to effectively implement bug classification in a software development team?\n",
      "Search for:  1. What is bug classification in software development?\n",
      "Search for:  2. What are the benefits of bug classification in software development?\n",
      "Search for:  3. How does bug classification improve software quality and efficiency?\n",
      "Search for:  What is bug classification? Why it is good?\n",
      "expended_result_docs 13\n",
      "Unique docs: 12\n",
      "Bug classification is a process in software development that involves categorizing software bugs based on their characteristics, severity, and impact on the software system. This process helps developers and quality assurance teams to prioritize and manage bugs more effectively.\n",
      "\n",
      "The reasons why bug classification is good for software development are:\n",
      "\n",
      "1. **Improved Efficiency**: By categorizing bugs, development teams can focus on the most critical issues first, ensuring that the most severe problems are addressed promptly. This prioritization helps to optimize the use of resources and improve overall efficiency.\n",
      "\n",
      "2. **Better Communication**: Bug classification provides a common language and understanding among team members about the nature and importance of different bugs. This improved communication helps to ensure that everyone is on the same page and working towards the same goals.\n",
      "\n",
      "3. **Enhanced Quality**: By effectively managing and resolving bugs, software development teams can improve the overall quality of their products. This leads to fewer issues for users to deal with and a better overall user experience.\n",
      "\n",
      "4. **Risk Management**: Bug classification helps to identify and prioritize bugs that pose the greatest risk to the software system or its users. This allows teams to address these risks before they become major problems, reducing the likelihood of costly downtime or data loss.\n",
      "\n",
      "5. **Continuous Improvement**: By analyzing the types of bugs that are most common in a software system, development teams can identify patterns and trends that may indicate underlying issues with the development process or the software architecture. This information can be used to make improvements and prevent similar bugs from occurring in the future.\n",
      "\n",
      "In summary, bug classification is a crucial aspect of software development that helps teams to manage and resolve bugs more effectively, leading to improved efficiency, better communication, enhanced quality, and continuous improvement.\n"
     ]
    }
   ],
   "source": [
    "# Finally query using RAG\n",
    "oroginal_query = \"What is bug classification? Why it is good?\"\n",
    "expanded_queries = query_expander(oroginal_query)\n",
    "expanded_queries.append(oroginal_query)\n",
    "\n",
    "expended_result_docs = []\n",
    "for query in expanded_queries:\n",
    "    print(\"Search for: \", query)\n",
    "    result_docs = retriever.invoke(query)\n",
    "    expended_result_docs.append(result_docs)\n",
    "\n",
    "# remove duplicates\n",
    "unique_docs = list(set(expanded_queries))\n",
    "print(\"expended_result_docs\", len(expended_result_docs))\n",
    "print(\"Unique docs:\", len(unique_docs))\n",
    "\n",
    "gc_result = chain.invoke(\n",
    "    {\"history\": history, \"context\": expanded_queries, \"input\": query}\n",
    ")\n",
    "print(gc_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of the Code: Smart Retrieval Augmented Generation (RAG)\n",
    "![smartRAG](./figures/a_in.png)\n",
    "\n",
    "### High-Level Overview\n",
    "\n",
    "The code demonstrates a smart Retrieval Augmented Generation (RAG) system that combines local retrieval with external search capabilities. The main goal is to provide relevant context for answering user questions by first searching a local vector database and then falling back to an external search service if the local context is insufficient.\n",
    "\n",
    "\n",
    "The code defines two main functions:\n",
    "\n",
    "\n",
    "  1. is_in: Determines whether the answer to a given question can be found within the provided context.\n",
    "smart_rag: Retrieves relevant context for a given question, either from the local vector database or an external search service, and generates an answer using the retrieved context.\n",
    "\n",
    "  1. The code uses the LangChain library for generating prompts and invoking language models, as well as the Tavily API for external search capabilities.\n",
    "\n",
    "\n",
    "### Detailed Explanation \n",
    "\n",
    "1. The code starts by defining the is_in function, which takes a question and context as input and determines whether the answer to the question can be found within the context.\n",
    "    * It defines a prompt template called is_in_conetxt that asks the language model to check if the answer is in the context and return \"yes\" or \"no\".\n",
    "    * The prompt template is used to create a ChatPromptTemplate object.\n",
    "    * A chain of operations is constructed using the | operator:\n",
    "      * The ChatPromptTemplate is passed to the ChatUpstage model.\n",
    "      * The model's output is parsed using the StrOutputParser.\n",
    "    * The chain is invoked with the question and context, and the response is stored in the response variable.\n",
    "    * The function returns True if the response starts with \"yes\" (case-insensitive), indicating that the answer is in the context.\n",
    "\n",
    "1. The code then demonstrates the usage of the is_in function with two example questions and their corresponding contexts retrieved from a retriever.\n",
    "\n",
    "1. Next, the code defines the smart_rag function, which takes a question as input and generates an answer using the retrieved context.\n",
    "    * It first retrieves the context for the question using the retriever.invoke method.\n",
    "    * If the is_in function determines that the answer is not in the retrieved context, it falls back to searching for additional context using the Tavily API.\n",
    "    * The retrieved context (either from the local retriever or Tavily) is stored in the context variable.\n",
    "    * A chain of operations is constructed using the | operator:\n",
    "      * The rag_with_history_prompt (not shown in the code snippet) is used as the prompt template.\n",
    "      * The prompt is passed to the llm language model.\n",
    "      * The model's output is parsed using the StrOutputParser.\n",
    "    * The chain is invoked with the conversation history, retrieved context, and the question, and the generated answer is returned.\n",
    "\n",
    "1. Finally, the code demonstrates the usage of the smart_rag function with two example questions:\n",
    "    * \"What is DUS?\": The answer is expected to be found in the local context.\n",
    "    * \"What's the population of San Francisco?\": The answer is not expected to be found in the local context, so it falls back to searching with Tavily.\n",
    "\n",
    "This code showcases how LangChain can be used to build a smart RAG system that combines local retrieval with external search capabilities. By first searching a local vector database and falling back to an external search service if needed, the system aims to provide relevant context for generating accurate answers to user questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG or Search?\n",
    "def is_in(question, context):\n",
    "    is_in_conetxt = \"\"\" please determine if the context includes relevent information from the question. \n",
    "If the answer for the question is present in the context, please respond with \"yes\". \n",
    "If not, please respond with \"no\". \n",
    "Only provide \"yes\" or \"no\" and avoid including any additional information. \n",
    "Please do your best. Here is the question and the context:\n",
    "---\n",
    "CONTEXT: {context}\n",
    "---\n",
    "QUESTION: {question}\n",
    "---\n",
    "OUTPUT (yes or no):\"\"\"\n",
    "\n",
    "    is_in_prompt = PromptTemplate.from_template(is_in_conetxt)\n",
    "    chain = is_in_prompt | ChatUpstage() | StrOutputParser()\n",
    "\n",
    "    response = chain.invoke({\"history\": [], \"context\": context, \"question\": question})\n",
    "    print(response)\n",
    "    return response.lower().startswith(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "question = \"Can you tell me about Yi Sun-sin, a Korean admiral?\"\n",
    "context = semantic_split_retriever.invoke(question)\n",
    "print(is_in(question, context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "question = \"How bug classification works?\"\n",
    "context = semantic_split_retriever.invoke(question)\n",
    "print(is_in(question, context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smart RAG, Self-Improving RAG\n",
    "from tavily import TavilyClient\n",
    "\n",
    "\n",
    "def smart_rag(question):\n",
    "    context = retriever.invoke(question)\n",
    "    if not is_in(question, context):\n",
    "        print(\"Searching in tavily\")\n",
    "        tavily = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n",
    "        context = tavily.search(query=question)\n",
    "\n",
    "    chain = rag_with_history_prompt | llm | StrOutputParser()\n",
    "    return chain.invoke({\"history\": history, \"context\": context, \"input\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n",
      "Searching in tavily\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Bug classification is a process in software development that involves categorizing software bugs based on their characteristics, severity, and impact on the software system. This process helps developers and quality assurance teams to prioritize and manage bugs more effectively.'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"bug classification?\"\n",
    "smart_rag(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n",
      "Searching in tavily\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Yi Sun-sin was a Korean admiral who lived during the 16th century. He is considered one of the most revered national heroes in Korea for his significant contributions to the country\\'s defense during the Japanese invasions of Korea (1592-1598). Here are some key points about his life and achievements:\\n\\n1. **Early Life**: Yi Sun-sin was born in 1545 in Jinju, Korea. He came from a family of military officials and was known for his intelligence and strategic thinking.\\n\\n2. **Military Career**: Yi served in various military positions before being appointed as the commander of the Left Navy Fleet in 1591. He was known for his innovative tactics and his development of the \"turtle ship,\" a type of armored warship that played a crucial role in his victories against the Japanese.\\n\\n3. **Battle of Hansan Island**: In 1592, during the first Japanese invasion, Yi led his fleet to victory at the Battle of Hansan Island. Despite being vastly outnumbered, he used his turtle ships and strategic maneuvers to destroy a significant portion of the Japanese fleet.\\n\\n4. **Imprisonment and Release**: Unfortunately, Yi was falsely accused of treason by his rivals and was imprisoned. However, after the Japanese launched a second invasion in 1597, he was released and reinstated as the commander of the fleet.\\n\\n5. **Battle of Myeongryang**: In 1597, during the second Japanese invasion, Yi led his fleet to another significant victory at the Battle of Myeongryang. Despite having only 13 ships against the Japanese fleet of over 130, he used his turtle ships and strategic maneuvers to destroy more than 30 Japanese ships.\\n\\n6. **Death**: Yi Sun-sin was fatally wounded during the Battle of Nojong in 1598 and died shortly after. His death was a significant loss for Korea, but his legacy lived on, and he became a symbol of Korean resistance against foreign aggression.\\n\\n7. **Legacy**: Yi Sun-sin is remembered as one of Korea\\'s greatest military leaders and a national hero. His achievements have been celebrated in various forms of Korean culture, including literature, film, and television. His strategic thinking and innovations in naval warfare continue to inspire people to this day.'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Can you tell me about Yi Sun-sin, a Korean admiral?\"\n",
    "smart_rag(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groundedness Check with LangChain and Upstage\n",
    "![Groundedness](./figures/gc.png)\n",
    "### High-Level Overview\n",
    "\n",
    "The provided code demonstrates how to perform a groundedness check using the LangChain library and the Upstage model. The groundedness check is a process of verifying whether the generated response is grounded in the given context. This is an important step in ensuring the quality and relevance of the generated output.\n",
    "\n",
    "The code uses the `UpstageGroundednessCheck` class from the `langchain_upstage` module to perform the groundedness check. It takes the context (a string of unique documents) and the generated response as input, and returns a verdict indicating whether the response is grounded or not.\n",
    "\n",
    "### Detailed Explanation\n",
    "\n",
    "1. The code starts by importing the necessary module:\n",
    "   - `UpstageGroundednessCheck` from `langchain_upstage`: This class is used to perform the groundedness check.\n",
    "\n",
    "2. An instance of the `UpstageGroundednessCheck` class is created and assigned to the variable `groundedness_check`.\n",
    "\n",
    "3. The input for the groundedness check is prepared by creating a dictionary called `request_input`:\n",
    "   - The `\"context\"` key is assigned the value of `str(unique_docs)`, which represents the unique documents as a string.\n",
    "   - The `\"answer\"` key is assigned the value of `response`, which represents the generated response.\n",
    "\n",
    "4. The `invoke` method of the `groundedness_check` instance is called with the `request_input` as an argument. This method performs the groundedness check and returns the verdict.\n",
    "\n",
    "5. The verdict is stored in the `response` variable and printed to the console using `print(response)`.\n",
    "\n",
    "6. The code then checks if the `response` starts with the word \"grounded\" (case-insensitive):\n",
    "   - If the response starts with \"grounded\", it means the groundedness check has passed, and the message \"✅ Groundedness check passed\" is printed.\n",
    "   - If the response does not start with \"grounded\", it means the groundedness check has failed, and the message \"❌ Groundedness check failed\" is printed.\n",
    "\n",
    "\n",
    "The provided code demonstrates a simple yet effective way to perform a groundedness check using LangChain and Upstage. By verifying whether the generated response is grounded in the given context, it helps ensure the quality and relevance of the output.\n",
    "\n",
    "Groundedness checks are an important step in building reliable and trustworthy language models and conversational agents. They help prevent the generation of irrelevant, inconsistent, or factually incorrect responses.\n",
    "\n",
    "By using the `UpstageGroundednessCheck` class from LangChain, developers can easily integrate groundedness checks into their language model pipelines and improve the overall performance of their systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grounded\n",
      "✅ Groundedness check passed\n"
     ]
    }
   ],
   "source": [
    "# GC\n",
    "from langchain_upstage import UpstageGroundednessCheck\n",
    "\n",
    "groundedness_check = UpstageGroundednessCheck()\n",
    "\n",
    "context = \"DUS is a new approach developed by Upstage to improve the search quality.\"\n",
    "answer = \"DUS is developed by Upstage.\"\n",
    "\n",
    "request_input = {\n",
    "    \"context\": context,\n",
    "    \"answer\": answer,\n",
    "}\n",
    "gc_result = groundedness_check.invoke(request_input)\n",
    "\n",
    "print(gc_result)\n",
    "if gc_result.lower().startswith(\"grounded\"):\n",
    "    print(\"✅ Groundedness check passed\")\n",
    "else:\n",
    "    print(\"❌ Groundedness check failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Groundedness check failed\n"
     ]
    }
   ],
   "source": [
    "context = \"DUS is a new approach developed by Upstage to improve the search quality.\"\n",
    "answer = \"DUS is developed by Google.\"\n",
    "\n",
    "request_input = {\n",
    "    \"context\": context,\n",
    "    \"answer\": answer,\n",
    "}\n",
    "gc_result = groundedness_check.invoke(request_input)\n",
    "\n",
    "if gc_result.lower().startswith(\"grounded\"):\n",
    "    print(\"✅ Groundedness check passed\")\n",
    "else:\n",
    "    print(\"❌ Groundedness check failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.ragas.io/en/stable/getstarted/testset_generation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Tools in LangChain\n",
    "\n",
    "### High-Level Overview\n",
    "\n",
    "The provided code demonstrates how to create custom tools in LangChain, a framework for developing applications powered by language models. Tools are essential components in LangChain that allow language models to perform specific tasks or access external resources.\n",
    "\n",
    "The code defines three custom tools:\n",
    "\n",
    "1. `add`: A tool that adds two integers.\n",
    "2. `multiply`: A tool that multiplies two integers.\n",
    "3. `get_news`: A tool that retrieves news articles on a given topic using an external API.\n",
    "\n",
    "These tools are then bound to a language model using the `bind_tools` method, enabling the model to utilize these tools when generating responses.\n",
    "\n",
    "### Detailed Explanation\n",
    "\n",
    "Let's break down the code and explain each part in detail:\n",
    "\n",
    "1. Importing necessary modules:\n",
    "   - `tool` from `langchain_core.tools`: This module provides the `@tool` decorator for defining custom tools.\n",
    "   - `requests`: A library for making HTTP requests to external APIs.\n",
    "\n",
    "2. Defining the `add` tool:\n",
    "   - The `@tool` decorator is used to define the `add` function as a custom tool.\n",
    "   - The function takes two integer parameters, `a` and `b`, and returns their sum.\n",
    "   - The docstring provides a brief description of the tool's functionality.\n",
    "\n",
    "3. Defining the `multiply` tool:\n",
    "   - Similar to the `add` tool, the `multiply` function is defined as a custom tool using the `@tool` decorator.\n",
    "   - It takes two integer parameters, `a` and `b`, and returns their product.\n",
    "   - The docstring describes the tool's purpose.\n",
    "\n",
    "4. Defining the `get_news` tool:\n",
    "   - The `get_news` function is defined as a custom tool using the `@tool` decorator.\n",
    "   - It takes a `topic` parameter of type `str` and returns news articles related to that topic.\n",
    "   - The function constructs a URL for the news API using the provided topic and an API key stored in an environment variable.\n",
    "   - It sends a GET request to the API using the `requests` library and returns the JSON response.\n",
    "\n",
    "5. Creating a list of tools:\n",
    "   - The `tools` list is created, containing the `add`, `multiply`, and `get_news` tools.\n",
    "   - This list will be used to bind the tools to the language model.\n",
    "\n",
    "6. Binding the tools to the language model:\n",
    "   - The `bind_tools` method of the `llm` object is called, passing the `tools` list as an argument.\n",
    "   - This step binds the custom tools to the language model, allowing it to utilize these tools when generating responses.\n",
    "   - The resulting object is assigned to the variable `llm_with_tools`.\n",
    "\n",
    "Conclusion\n",
    "\n",
    "The code demonstrates how to create custom tools in LangChain, which can be used to extend the capabilities of language models. By defining tools for specific tasks, such as mathematical operations or retrieving news articles, developers can enhance the functionality of their LangChain applications.\n",
    "\n",
    "The `@tool` decorator simplifies the process of defining custom tools, while the `bind_tools` method allows seamless integration of these tools with the language model.\n",
    "\n",
    "By leveraging custom tools, LangChain enables developers to build powerful and versatile applications that can perform a wide range of tasks beyond simple text generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tools\n",
    "from langchain_core.tools import tool\n",
    "import requests\n",
    "\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds a and b.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiplies a and b.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_news(topic: str) -> str:\n",
    "    \"\"\"Get news on a given topic.\"\"\"\n",
    "    # https://newsapi.org/v2/everything?q=tesla&from=2024-04-01&sortBy=publishedAt&apiKey=API_KEY\n",
    "    # change this to request news from a real API\n",
    "    news_url = f\"https://newsapi.org/v2/everything?q={topic}&apiKey={os.environ['NEWS_API_KEY']}\"\n",
    "    respnse = requests.get(news_url)\n",
    "    return respnse.json()\n",
    "\n",
    "\n",
    "tools = [add, multiply, get_news]\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_tool(tool_call):\n",
    "    tool_name = tool_call[\"name\"].lower()\n",
    "    if tool_name not in globals():\n",
    "        print(\"Tool not found\", tool_name)\n",
    "        return None\n",
    "    selected_tool = globals()[tool_name]\n",
    "    return selected_tool.invoke(tool_call[\"args\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'multiply', 'args': {'a': 3, 'b': 12}, 'id': '0f910fb7-449b-44a7-ac32-6be80e43b2a8'}, {'name': 'add', 'args': {'a': 11, 'b': 49}, 'id': '9a413b4e-eb49-48de-80fc-a0c4c8f11327'}]\n"
     ]
    }
   ],
   "source": [
    "query = \"What is 3 * 12? Also, what is 11 + 49?\"\n",
    "\n",
    "tool_calls = llm_with_tools.invoke(query).tool_calls\n",
    "print(tool_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "for tool_call in tool_calls:\n",
    "    print(call_tool(tool_call))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'get_news', 'args': {'topic': 'NewJeans'}, 'id': '20c8f08a-78f2-46df-9672-817b1514b8f0'}]\n",
      "{'status': 'ok', 'totalResults': 282, 'articles': [{'source': {'id': None, 'name': 'ReadWrite'}, 'author': 'Paul McNally', 'title': '‘Most important girl group on the planet’ coming to PUBG Battlegrou\n"
     ]
    }
   ],
   "source": [
    "query = \"What's news on NewJeans?\"\n",
    "\n",
    "tool_calls = llm_with_tools.invoke(query).tool_calls\n",
    "print(tool_calls)\n",
    "\n",
    "for tool_call in tool_calls:\n",
    "    print(str(call_tool(tool_call))[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardrails\n",
    "\n",
    "Guardrails is a Python framework that helps build reliable AI applications by performing two key tasks. It runs Input/Output Guards in your application to detect, measure, and reduce specific risks. See Guardrails Hub for the full list of risks. For more info, visit <https://www.guardrailsai.com/docs>.\n",
    "![Guardrails](./figures/gr.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/guardrails-ai/validator-template/blob/main/validator/main.py\n",
    "from typing import Any, Callable, Dict, Optional\n",
    "from guardrails import OnFailAction\n",
    "from guardrails.validator_base import (\n",
    "    FailResult,\n",
    "    PassResult,\n",
    "    ValidationResult,\n",
    "    Validator,\n",
    "    register_validator,\n",
    ")\n",
    "\n",
    "\n",
    "@register_validator(name=\"guardrails/solar_validator\", data_type=\"string\")\n",
    "class SolarValidator(Validator):\n",
    "    \"\"\"Validates that {fill in how you validator interacts with the passed value}.\"\"\"\n",
    "\n",
    "    # If you don't have any init args, you can omit the __init__ method.\n",
    "    def __init__(\n",
    "        self,\n",
    "        keyword: str = \"solar\",\n",
    "        on_fail: Optional[Callable] = OnFailAction.EXCEPTION,\n",
    "    ):\n",
    "        super().__init__(on_fail=on_fail, keyword=keyword)\n",
    "        self._keyword = keyword\n",
    "\n",
    "    def validate(self, value: Any, metadata: Dict = {}) -> ValidationResult:\n",
    "        \"\"\"Validates that {fill in how you validator interacts with the passed value}.\"\"\"\n",
    "        # Add your custom validator logic here and return a PassResult or FailResult accordingly.\n",
    "        if self._keyword in str(value):\n",
    "            return PassResult()\n",
    "\n",
    "        return FailResult(\n",
    "            error_message=\"{A descriptive but concise error message about why validation failed}\",\n",
    "            fix_value=\"{The programmtic fix if applicable, otherwise remove this kwarg.}\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from guardrails import Guard\n",
    "\n",
    "guard = Guard().use(SolarValidator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ValidationOutcome(raw_llm_output='solar llm is super great!', validated_output='solar llm is super great!', reask=None, validation_passed=True, error=None)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guard.parse(\"solar llm is super great!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guard failed: Validation failed for field with errors: {A descriptive but concise error message about why validation failed}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    guard.parse(\"My favorite phone is BlackBerry.\")\n",
    "except Exception as e:\n",
    "    print(\"Guard failed:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Exciting Excercise: Building Your Own AI-Powered Chatbot! 🤖\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Congratulations on completing the course on building chatbots using Language Models (LLMs), Layout Analysis (LA), custom tools, and Groundedness Checks (GC)! It's time to put your skills to the test by creating your own AI-powered chatbot. 🎉\n",
    "\n",
    "### Objective\n",
    "\n",
    "Your task is to develop a chatbot that can perform various tasks based on user queries, such as:\n",
    "\n",
    "- 🎨 Drawing images based on user descriptions\n",
    "- 📰 Searching for the latest news on various topics\n",
    "- 📅 Checking and managing schedules\n",
    "- 📄 Extracting structured information from PDFs and images using Layout Analysis\n",
    "- 🌟 And more!\n",
    "\n",
    "### Requirements\n",
    "\n",
    "To create your chatbot, you'll need to leverage the following components:\n",
    "\n",
    "1. 🧠 Language Model (LLM): Use a powerful LLM to understand user queries and generate responses.\n",
    "\n",
    "2. 📊 Layout Analysis (LA): Utilize Layout Analysis techniques to extract structured information from PDFs and images.\n",
    "\n",
    "3. 🛠️ Custom Tools: Develop custom tools for specific actions like image generation, news search, and schedule management.\n",
    "\n",
    "4. ✅ Groundedness Check (GC): Implement a groundedness check to ensure relevant and accurate responses.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This homework assignment is your opportunity to showcase your skills in building an AI-powered chatbot that can understand and process visual content using Layout Analysis. Have fun and be creative! 🚀\n",
    "\n",
    "Happy coding, and may your chatbot impress everyone! 😄"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
